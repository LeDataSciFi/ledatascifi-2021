{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "3. **15 min: Go over my finished example from the last lecture (short code in discussion repo)**\n",
    "    - open my participation folder and wiki_scraper\n",
    "    - start it running then, talk through code\n",
    "    - open up github desktop and see the 505 files trying to upload... let's gitignore them. they disappear! sync to GH\n",
    "    \n",
    "4. **15 min: Group discussion about approach to getting URLs and saving 10ks for assignment:**\n",
    "    - poll: who was able to download all 148?\n",
    "    - me: open the new datafile I gave them, and sample 20 rows for the new tech and edgar vars\n",
    "    - so: \n",
    "        - what did they/can they use as the url?\n",
    "        - what the folder structure they used? (they can just all use mine on the assignment, but try to get them to propose other org structures and talk about pros/cons.)\n",
    "            - e.g. a decent rule is to only use info available in the data. \n",
    "            - wouldn't it make sense to add a layer of folders to denote the filing year for humans as they scroll the directory? yeah, probably actually! \n",
    "            - how about separating by form type? yeah, probably actually!\n",
    "        - what is the pathname?\n",
    "            - i used the webpage... maybe they hard code the filing form/firm/year into the name if not the folder structure\n",
    "        - pros and cons are about \n",
    "            - how easy it is to code,\n",
    "            - how robust to weird situations (like repeat filings!!!), \n",
    "            - how easily a human scanner can figure out org/find docs they want. \n",
    "        - in my scheme, users would basically need the input data and to search for the filing name (ouch) or open and read each doc (OUUUUUCH)\n",
    "        \n",
    "5. **Rest of class: CONCEPTUAL (NO CODE)**\n",
    "\n",
    "So most of you will still have hard drives that are in GBs. The largest data file we've messed with yet is way less than a GB. And the SEC files for this assignment are the biggest yet, but they total less than 25% of GB. I have 512 GB on my comp. So imagine analyzing 1000GB - that seems like a lot! 1000 GB is called a terabyte...\n",
    "\n",
    "...and IBM estiamted 3 years ago that the internet generated 2.5 MILLION TERABYTES of data EVERY DAY. (!!!)\n",
    "\n",
    "Since then, hundreds of millions of people have gotten internet access and the rest use more internet than before. We're talking about posts, text messages, likes, searches, and emails which our devices automatically encode with location data. \n",
    "\n",
    "As of 2020, there is 40 ZETTABYTES of web data. A zettabyte is a billion terabytes. (DAMN). A zettabyte is about how many stars are in the observable universe. \n",
    "\n",
    "That's crazy. SO back to the data. \n",
    "\n",
    "Most of that data is \"unstructured\" text data. Something like 80% of it. Figuring out how to put structure on that data is POWERFUL. Google is the most famous example - they figured one way to structure that data, which was to rank documents and then give you the ranked list based on whether your search words were in the document. \n",
    "\n",
    "There are so so so many questions to pose of the world of data and each question needs a different approach than the last. \n",
    "\n",
    "One very common task is identifying topics in unstructured text (does the document discuss it at all) and evaluating what a document says about those topics - are ABSOLUTELY substantially utterly megaly important for firms around the finance and business world. \n",
    "\n",
    "**As a group:** Discuss how we can identify a topic in a text file**\n",
    "   - let's pick a topic as a class...\n",
    "   - have them open a 10K and read it to see if the firm discusses it and what you learn about (no/small/large exposure)\n",
    "   - Q1: how can we translate the way we looked for that risk in the document into steps/rules a computer can follow?\n",
    "   - Q2: how can we try to encode the \"size\" of exposure?   \n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
