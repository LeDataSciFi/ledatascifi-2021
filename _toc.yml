# Table of contents
#
# possible external link:
# - url: http://espn.com 
#
# Can override a title:
# title: A special title  
#
# Learn more at https://jupyterbook.org/customize/toc.html

- file: content/frontpage # the index must be a single file, no way around it!


- part: About the class
  chapters:
  - file: content/about/objectives
  - file: content/about/outcomes
    title: "Outcomes"
  - file: content/about/about_us
    title: "About us + office hours" 
  - file: content/about/structure_and_policies
  - file: content/about/gradeoverview
  - file: content/about/acknowledgments
  
- part: Schedule, tips, resources
  chapters:
  - file: content/about/schedule
    title: "Dashboard, key links, schedule"
  - file: content/about/resources
    title: "Help + resources"
  - file: content/about/tips


- part: Assignments and participation  
  chapters:
  - file: content/assignments/howto_do
    title: "How to start and turn in assignments"
  - file: content/assignments/howto_review 
    title: "How to do your peer reviews"
  - file: content/assignments/asgn_5 
    title: "ASGN 5: Our first full data science assignment"
  
- part: Textbook 
  numbered: true # will number the chapters and sections INSIDE the part(s) below
  chapters:  
  - file: "content/01/00_Getting_Started"
    title: "Motivation and Getting Started"
    sections:
    - file: "content/01/01_Motivation"
      title: "Motivation"
    - file: "content/01/02_Setup"
    - file: "content/01/03_github"
    - file: "content/01/04_Markdown"
    - file: "content/01/05_jupyterlab"
      title: "Jupyter Lab Basics"    
    - file: "content/01/06_python"
      title: "Python Basics"    
    - file: "content/01/07_debugging"
      title: "Debugging"    
    - file: "content/01/07a_errors"
    - file: "content/01/08_libraries"
    - file: "content/01/09_gitignore"
      title: "Gitignore Files"    
    
  - file: "content/02/10_Golden_1"
    title: "Good Analysis Practices"
    sections:
    - file: "content/02/10_Golden_2"
      title: "A case study of bad research"
    - file: "content/02/10_Golden_3"
      title: "The golden rules"
    - file: "content/02/10_Golden_4"
      title: "Good data"
    - file: "content/02/10_Golden_5"
      title: "When to write functions"
    - file: "content/02/10_Golden_6"
      title: "Reorganizing the bad research folder"
    - file: "content/02/10_Golden_7"
      title: "Filepaths"
    
  - file: "content/03/00_Data_Wrangling_Intro"
    title: "Wrangling with Data"
    sections:
    - file: "content/03/01_Numpy"
      title: "Numpy"
      sections:
        - file: "content/03/01a_NumpyCS"
          title: "Numpy + Scientific Computing"
        - file: "content/03/01b_NumpyBasics"
          title: "A (Very) Short Introduction"
        - file: "content/03/01c_NumpyPractice"
          title: "Exercises"
        - file: "content/03/01d_NumpyResources"
          title: "More Resources"
    - file: "content/03/02a_pandasIntro"
      title: "Pandas"
      sections:
        - file: "content/03/02a_pandasTips"
          title: "Tips"      
        - file: "content/03/02b_pandasVocab"
          title: "Vocab and Long vs Wide Data"      
        - file: "content/03/02c_commonFcns"
          title: "Common Functions"      
        - file: "content/03/02d_temp"
          title: "Temp. vs Perm. Objects"      
        - file: "content/03/02e_eda_golden"
          title: "Golden Rules + EDA"      
        - file: "content/03/02f_chains"
          title: "Pandas Chains" 
        - file: "content/03/02g_commontasks"
          title: "Common Tasks" 
        - file: "content/03/02h_exercises"
          title: "Exercises" 
        - file: "content/03/02j_resourcesAndSum"
          title: "Summary and Resources" 
    - file: "content/03/04a-dataviz"
      title: "Data Visualization"
      sections:
        - file: "content/03/04b-whyplot"
        - file: "content/03/04c-makeplot"
        - file: "content/03/04d-whichplot"
        - file: "content/03/04e-visualEDA"
        - file: "content/03/04f-betterplots"
#        - file: "content/03/04g-lyingfigures"
    - file: "content/03/05a-otherskills"
      sections:
        - file: "content/03/05b_merging"
        - file: "content/03/05c_missingdata"
        - file: "content/03/05d_outliers"
     
  - file: "content/04/00_World_Wide_Data"
    title: "Accessing the World of Data"
    sections:
    - file: "content/04/01_Intro_to_scraping"
      title: "Getting Data off the Web"
    - file: "content/04/01a_openingAndParsing"
    - file: "content/04/01b_spiders"
      title: "Building a spider" 
    - file: "content/04/02_strings"
      sections:
        - file: "content/04/02a_Python Strings"
        - file: "content/04/02b_regex"
        - file: "content/04/02c_developing a regex"
        - file: "content/04/02d_RegexApplication"

  - file: "content/05/00_intro"
    title: "Data Science for Finance"    
    sections:
    - file: "content/05/01_bigpicture"
      sections:
        - file: "content/05/01a_MLgonewrong"    
        - file: "content/05/01b_model_process"
        - file: "content/05/01c_teams"
        - file: "content/05/01d_sharingBigFiles"
    - file: "content/05/02_reg"
      sections:
        - file: "content/05/02a_basics"    
        - file: "content/05/02b_mechanics"
        - file: "content/05/02c_goodnessOfFit"      
        - file: "content/05/02d_interpretingCoefs" 
        - file: "content/05/02e_statisticalSig"     
        # (opt, future?) selection, measurement, omitted variables (HIGH level)
        # (opt, future?) exercises (and answers)
        - file: "content/05/02g_summary"           
    - file: "content/05/03_ML"
      sections:
        - file: "content/05/03a_ML_obj_and_tradeoff"    
        - file: "content/05/03c_ModelEval" 
        - file: "content/05/03c1_OOS"  
        - file: "content/05/03d_whatToMax"   # need auc/roc
        - file: "content/05/03e_whichModel"  # need lots more!   
# illustration here (not code focused )        
# est stock return predictor and get predicted performance R2 in sample
# say 2000-2006 returns
# now get shitty OOS R2 when actually applied to 2007-2009      
    - file: "content/05/04a_SKLearn"        
      sections:
        - file: "content/05/04b_best_practices" # add extra section for best practices
          title: "Best Practice Pseudo Code"        
        - file: "content/05/04c_onemodel"
          title: "SKLearn Intro"                             
        - file: "content/05/04d_crossval"
          title: "Cross-Validation"
        - file: "content/05/04e_pipelines"
          title: "Pipelines"
        - file: "content/05/04e1_preprocessing"
          title: "Preprocessing"          
        - file: "content/05/04f_optimizing_a_model"
          title: "Optimizing a Model" # see notes below
        - file: "content/05/04h_putting_together"
          title: "All at Once" # see notes below
          # boosting page?
          # import smattering?

# #########################################################
# o4f

# #             1. gridsearch to optimize pipeline parameters
# #                 - set up grid (print pipe) moving grid around
# #                 - local vs global max (graph)
# #                 - saving best model to fit it
# #                 - use best model on test split
                                
# #             L23 fir students has some good stuff

# #########################################################


# #         - file: "content/05/04g_leakage_disaster"
# #           title: "leakage disaster"
# # 
# #    "Leakage disasters"
# #        yahoo download 200 days of returns for a firm, create returns, then create predictive model (simple lags). then evaluate OOS
# #        then do REAL exercise without future info - OOS will be WAY worse
# #    pipeline solution

# #########################################################

# #         - file: "content/05/04h_putting_together"
# #           title: "04h_putting_together"

# #                 An example with:

#                     # import lots of functions
#                     # load data 
#                     # split to test and train (link to split page/sk docs)
                    
#                     ## pre-modeling (on the training data only!)
                    
#                     # do lots of EDA
#                     # look for missing values, which variables are what type, and outliers (copy from a6 prompt)
#                     # figure out how you'd clean the data (imputation, scaling, encoding categorical vars)
#                     # these lessons will go into the preprocessign portion of your pipeline 

#                     ## optimize a series of models 

#                     # set up pipeline to clean each type of variable (1 pipe per var type)
#                     # combine those pipes into "preprocess" pipe
#                     # set up cv (can set up iterable to do OOS! or TimeSeriesSplit, or Snows(?))
#                     # set up scoring 

#                     ## optimize candidate model type #1: (link to optimizing one model page   04d_optimizing_a_model)
                    
#                     #     set up pipeline (combines preprocessing, estimator)
#                     #     set up hyper param grid
#                     #     find optimal hyper params (e.g. gridsearchcv)
#                     #     save pipeline with optimal params in place
#                     #     (Note: you should spend time interrogating model predictions, plotting and printing
#                     #     does the model struggle predicting certain obs? excel at some?)
                   
#                     ## optimize candidate model type #2
                    
#                     ...
                    
#                     ## optimize candidate model type #N

#                     ## compare the N optimized models

#                     # build list of models (each with own optimized hyperparams)
#                     # for model in models:
#                     #    cross_validate(model, X, y,...)
#                     # pick the winner!


# #########################################################

# # gradient booksting from L24

# #########################################################


# # ## A bunch of import statements 

# # Any code you write might use none of these, some of these, or all of them!

# # # dataset loader
# # from sklearn import datasets

# # # model training and evalutation utilities 
# # from sklearn.model_selection import train_test_split
# # from sklearn.model_selection import cross_validate, GridSearchCV
# # from sklearn.model_selection import StratifiedKFold # this is one way to generate folds
# # from sklearn.model_selection import KFold

# # # metrics
# # from sklearn import metrics
# # from sklearn.metrics import r2_score
# # from sklearn.metrics import accuracy_score
# # from sklearn.metrics import classification_report
# # from sklearn.metrics import confusion_matrix

# # # preprocessing and feature extraction
# # from sklearn.pipeline import Pipeline, make_pipeline
# # from sklearn.compose import ColumnTransformer, make_column_selector
# # from sklearn import preprocessing
# # from sklearn.preprocessing import StandardScaler, OneHotEncoder
# # from sklearn.feature_extraction import DictVectorizer
# # from sklearn.impute import SimpleImputer
# # from df_after_transform import df_after_transform # df_after_transform.py must be in this folder

# # # feature selection
# # from sklearn.linear_model import ElasticNet, Lasso, Ridge
# # from sklearn.feature_selection import SelectKBest, RFE, RFECV,

# # # models
# # from sklearn.svm import SVC
# # from sklearn.neighbors import KNeighborsClassifier
# # from sklearn.linear_model import LinearRegression
# # from sklearn import linear_model
# # from sklearn.linear_model import LogisticRegression
# # from sklearn.tree import DecisionTreeClassifier
# # from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
# # from sklearn.naive_bayes import GaussianNB

# # # toy data
# # X, y = datasets.load_iris(return_X_y=True)
# # X.shape, y.shape






